{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from cs_opl import op_learning\n",
    "from experiment_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py:224: RuntimeWarning: invalid value encountered in true_divide\n",
      "  G = G_numer / G_denom\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py:236: RuntimeWarning: invalid value encountered in true_divide\n",
      "  B_x = d_mx / f_x - G * d_fx / f_x\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py:237: RuntimeWarning: invalid value encountered in true_divide\n",
      "  B_x = (G_numer * d_fx - G_denom * d_mx) / (G_denom**2)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py:237: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  B_x = (G_numer * d_fx - G_denom * d_mx) / (G_denom**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 0.4669254573857532\n",
      "score 0.5395483307332868\n",
      "score0 0.46585172771677347\n",
      "score 0.5382266618255738\n",
      "score0 0.4579309747243173\n",
      "score 0.5284905042926437\n",
      "score0 0.4022764843260414\n",
      "score 0.4601583354777361\n",
      "score0 0.15860379897450028\n",
      "score 0.16180681045631579\n",
      "score0 0.12871651956376695\n",
      "score 0.1268714955391437\n",
      "score0 0.12649786214938954\n",
      "score 0.12434666537339593\n",
      "score0 0.12628143621588062\n",
      "score 0.1241009794961791\n",
      "score0 0.5003578089859789\n",
      "score 0.4058766745926748\n",
      "score0 0.49927493641740167\n",
      "score 0.4050604019127549\n",
      "score0 0.49123920853203235\n",
      "score 0.39900526737639075\n",
      "score0 0.4347881141520482\n",
      "score 0.3564688269024201\n",
      "score0 0.16605405966216394\n",
      "score 0.15399046022976204\n",
      "score0 0.12915071967036512\n",
      "score 0.12615781486776984\n",
      "score0 0.12656024442801875\n",
      "score 0.12419261839238696\n",
      "score0 0.12630865626711202\n",
      "score 0.1240015829001806\n",
      "score0 0.5123494006746816\n",
      "score 0.35794760692492555\n",
      "score0 0.5112589086258681\n",
      "score 0.35736685219241493\n",
      "score0 0.5031644896278222\n",
      "score 0.35304808514107894\n",
      "score0 0.44629032937871344\n",
      "score 0.3225509645718552\n",
      "score0 0.1640073814751537\n",
      "score 0.1699966985154515\n",
      "score0 0.12445161598829868\n",
      "score 0.14483070216037167\n",
      "score0 0.12186251402728646\n",
      "score 0.14296713452156024\n",
      "score0 0.1216123706781022\n",
      "score 0.1427850488071596\n",
      "score0 0.4534635710581567\n",
      "score 0.5933695014103285\n",
      "score0 0.4523952232401991\n",
      "score 0.59183427980809\n",
      "score0 0.4445177137223469\n",
      "score 0.5805322467091947\n",
      "score0 0.38917763811976375\n",
      "score 0.5012622255714371\n",
      "score0 0.155221608441792\n",
      "score 0.16754434839363552\n",
      "score0 0.12764335367423613\n",
      "score 0.13077690047140594\n",
      "score0 0.12555231758799495\n",
      "score 0.12809245945562076\n",
      "score0 0.12534800354675277\n",
      "score 0.1278310939071235\n",
      "score0 0.47419375395042807\n",
      "score 0.5105334147053902\n",
      "score0 0.47310876618862785\n",
      "score 0.5092357782280365\n",
      "score0 0.4651799892795682\n",
      "score 0.49981113461203525\n",
      "score0 0.4094653515921517\n",
      "score 0.43382958443049113\n",
      "score0 0.16376418499059792\n",
      "score 0.14529186625385285\n",
      "score0 0.13228009623330406\n",
      "score 0.11300002434605615\n",
      "score0 0.12991207589411885\n",
      "score 0.11072746876499927\n",
      "score0 0.1296809141680041\n",
      "score 0.1105068266141022\n",
      "score0 0.4669254573857532\n",
      "score 0.5395483307332868\n",
      "score0 0.46585172771677347\n",
      "score 0.5382266618255738\n",
      "score0 0.4579309747243173\n",
      "score 0.5284905042926437\n",
      "score0 0.4022764843260414\n",
      "score 0.4601583354777361\n",
      "score0 0.15860379897450028\n",
      "score 0.16180681045631579\n",
      "score0 0.12871651956376695\n",
      "score 0.1268714955391437\n",
      "score0 0.12649786214938954\n",
      "score 0.12434666537339593\n",
      "score0 0.12628143621588062\n",
      "score 0.1241009794961791\n",
      "score0 0.5003578089859789\n",
      "score 0.4058766745926748\n",
      "score0 0.49927493641740167\n",
      "score 0.4050604019127549\n",
      "score0 0.49123920853203235\n",
      "score 0.39900526737639075\n",
      "score0 0.4347881141520482\n",
      "score 0.3564688269024201\n",
      "score0 0.16605405966216394\n",
      "score 0.15399046022976204\n",
      "score0 0.12915071967036512\n",
      "score 0.12615781486776984\n",
      "score0 0.12656024442801875\n",
      "score 0.12419261839238696\n",
      "score0 0.12630865626711202\n",
      "score 0.1240015829001806\n",
      "score0 0.5123494006746816\n",
      "score 0.35794760692492555\n",
      "score0 0.5112589086258681\n",
      "score 0.35736685219241493\n",
      "score0 0.5031644896278222\n",
      "score 0.35304808514107894\n",
      "score0 0.44629032937871344\n",
      "score 0.3225509645718552\n",
      "score0 0.1640073814751537\n",
      "score 0.1699966985154515\n",
      "score0 0.12445161598829868\n",
      "score 0.14483070216037167\n",
      "score0 0.12186251402728646\n",
      "score 0.14296713452156024\n",
      "score0 0.1216123706781022\n",
      "score 0.1427850488071596\n",
      "score0 0.4534635710581567\n",
      "score 0.5933695014103285\n",
      "score0 0.4523952232401991\n",
      "score 0.59183427980809\n",
      "score0 0.4445177137223469\n",
      "score 0.5805322467091947\n",
      "score0 0.38917763811976375\n",
      "score 0.5012622255714371\n",
      "score0 0.155221608441792\n",
      "score 0.16754434839363552\n",
      "score0 0.12764335367423613\n",
      "score 0.13077690047140594\n",
      "score0 0.12555231758799495\n",
      "score 0.12809245945562076\n",
      "score0 0.12534800354675277\n",
      "score 0.1278310939071235\n",
      "score0 0.47419375395042807\n",
      "score 0.5105334147053902\n",
      "score0 0.47310876618862785\n",
      "score 0.5092357782280365\n",
      "score0 0.4651799892795682\n",
      "score 0.49981113461203525\n",
      "score0 0.4094653515921517\n",
      "score 0.43382958443049113\n",
      "score0 0.16376418499059792\n",
      "score 0.14529186625385285\n",
      "score0 0.13228009623330406\n",
      "score 0.11300002434605615\n",
      "score0 0.12991207589411885\n",
      "score 0.11072746876499927\n",
      "score0 0.1296809141680041\n",
      "score 0.1105068266141022\n",
      "score0 0.46692540164605645\n",
      "score 0.5395482746251028\n",
      "score0 0.4658517027228265\n",
      "score 0.538226628947362\n",
      "score0 0.4579309677383468\n",
      "score 0.5284904948075032\n",
      "score0 0.4022764743028904\n",
      "score 0.4601583235872446\n",
      "score0 0.15860379479039122\n",
      "score 0.1618068055420723\n",
      "score0 0.12871651958755154\n",
      "score 0.12687149556448837\n",
      "score0 0.12649786214941117\n",
      "score 0.12434666537105904\n",
      "score0 0.12628143621588372\n",
      "score 0.12410097949618099\n",
      "score0 0.5003586333093364\n",
      "score 0.40587726444594047\n",
      "score0 0.4992749677917344\n",
      "score 0.4050604190527938\n",
      "score0 0.49123920618679484\n",
      "score 0.399005265884811\n",
      "score0 0.4347881132492613\n",
      "score 0.3564688256423481\n",
      "score0 0.1660540578771879\n",
      "score 0.1539904588974147\n",
      "score0 0.12915071969184544\n",
      "score 0.12615781489237207\n",
      "score0 0.12656024442807176\n",
      "score 0.1241926183945008\n",
      "score0 0.12630865626711396\n",
      "score 0.12400158290018089\n",
      "score0 0.5123478244259735\n",
      "score 0.3579465505522542\n",
      "score0 0.5112589535711707\n",
      "score 0.35736689300254254\n",
      "score0 0.5031644940205686\n",
      "score 0.3530480827859095\n",
      "score0 0.4462903345191087\n",
      "score 0.3225509675655375\n",
      "score0 0.16400738238611837\n",
      "score 0.16999669916215673\n",
      "score0 0.12445161596610424\n",
      "score 0.14483070216336844\n",
      "score0 0.12186251402831477\n",
      "score 0.1429671345241893\n",
      "score0 0.12161237067810389\n",
      "score 0.14278504880716028\n",
      "score0 0.4534637308104494\n",
      "score 0.5933696809885425\n",
      "score0 0.45239522179099695\n",
      "score 0.5918342775684432\n",
      "score0 0.444517714151365\n",
      "score 0.5805322465601495\n",
      "score0 0.38917763486883294\n",
      "score 0.501262221446225\n",
      "score0 0.15522160643475416\n",
      "score 0.16754434505670923\n",
      "score0 0.127643353625309\n",
      "score 0.1307769004132482\n",
      "score0 0.12555231758770496\n",
      "score 0.12809245945589004\n",
      "score0 0.1253480035467574\n",
      "score 0.12783109390712802\n",
      "score0 0.47419303704198057\n",
      "score 0.5105315914148484\n",
      "score0 0.4731087646502985\n",
      "score 0.5092357804626548\n",
      "score0 0.46517999679388544\n",
      "score 0.4998111425271516\n",
      "score0 0.4094653221896761\n",
      "score 0.43382955037368687\n",
      "score0 0.16376418081568447\n",
      "score 0.14529186170366437\n",
      "score0 0.1322800961819171\n",
      "score 0.11300002431907033\n",
      "score0 0.12991207589312315\n",
      "score 0.11072746876338803\n",
      "score0 0.12968091416801075\n",
      "score 0.11050682661410885\n",
      "score0 0.7566756751069345\n",
      "score 0.70677553250611\n",
      "score0 0.7523348584860172\n",
      "score 0.7064971068163322\n",
      "score0 0.7287628026850881\n",
      "score 0.698155276764026\n",
      "score0 0.6250622809408678\n",
      "score 0.6407672596411733\n",
      "score0 0.3819940056751827\n",
      "score 0.4593966525568703\n",
      "score0 0.1499873388014002\n",
      "score 0.15548117904330538\n",
      "score0 0.12801580267432897\n",
      "score 0.1263626117073956\n",
      "score0 0.12642887654074159\n",
      "score 0.12429655624265523\n",
      "score0 0.756657988632911\n",
      "score 0.7263527258895373\n",
      "score0 0.7512103681485104\n",
      "score 0.7158949272706212\n",
      "score0 0.7240749612223\n",
      "score 0.6890139043966098\n",
      "score0 0.6226497527380069\n",
      "score 0.5220555372424175\n",
      "score0 0.41863494763567577\n",
      "score 0.3452172543202325\n",
      "score0 0.15734611445567814\n",
      "score 0.14800608106083343\n",
      "score0 0.12845461127936622\n",
      "score 0.12566539003502628\n",
      "score0 0.12649180132439697\n",
      "score 0.12414402812804183\n",
      "score0 0.7284416048754433\n",
      "score 0.8442008950714321\n",
      "score0 0.7230322664597386\n",
      "score 0.8260149640083869\n",
      "score0 0.6971771955975679\n",
      "score 0.7827130622292375\n",
      "score0 0.6052589024900288\n",
      "score 0.6236262016866977\n",
      "score0 0.41599216090649777\n",
      "score 0.3342846877751737\n",
      "score0 0.1559606450164263\n",
      "score 0.16300402118587684\n",
      "score0 0.12391201646708966\n",
      "score 0.1441935798991668\n",
      "score0 0.12181004670743996\n",
      "score 0.14290373818206378\n",
      "score0 0.7506964836656029\n",
      "score 0.766522215180126\n",
      "score0 0.7443866517096009\n",
      "score 0.7644215442290384\n",
      "score0 0.7146757077345178\n",
      "score 0.7493726902637943\n",
      "score0 0.6144433423346389\n",
      "score 0.6626579587518013\n",
      "score0 0.378441538946662\n",
      "score 0.46625484144395773\n",
      "score0 0.1486972016579231\n",
      "score 0.15953635239717082\n",
      "score0 0.12705761285177117\n",
      "score 0.13012233957232733\n",
      "score0 0.12549424297785616\n",
      "score 0.12802804733926393\n",
      "score0 0.7769646055197379\n",
      "score 0.6492196910133647\n",
      "score0 0.7715822040439809\n",
      "score 0.6429868073590516\n",
      "score0 0.7439288453719044\n",
      "score 0.624025632948404\n",
      "score0 0.6440502735639211\n",
      "score 0.5367853096697273\n",
      "score0 0.40675590706810316\n",
      "score 0.380486546790051\n",
      "score0 0.15752997535111302\n",
      "score 0.13801263089929772\n",
      "score0 0.13165155230983908\n",
      "score 0.11244102486470574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 0.1298494413622674\n",
      "score 0.11067274823414902\n",
      "score0 0.46708164360586285\n",
      "score 0.539735549645602\n",
      "score0 0.4670800341908141\n",
      "score 0.5397326583357185\n",
      "score0 0.4670017433707066\n",
      "score 0.5396444112044163\n",
      "score0 0.4663917156233723\n",
      "score 0.5389089199307322\n",
      "score0 0.4618248432976146\n",
      "score 0.5334118007667044\n",
      "score0 0.42942164626919976\n",
      "score 0.49417097134432136\n",
      "score0 0.211301978350114\n",
      "score 0.22645051432431546\n",
      "score0 0.13089465321759455\n",
      "score 0.129415473269901\n",
      "score0 0.5005180401724957\n",
      "score 0.40599453024173954\n",
      "score0 0.5005191941494905\n",
      "score 0.4059960589760875\n",
      "score0 0.5004388997571184\n",
      "score 0.4059302679825671\n",
      "score0 0.4998243347654348\n",
      "score 0.4054196841107454\n",
      "score0 0.4952189184239719\n",
      "score 0.4017272673695421\n",
      "score0 0.46248060762392956\n",
      "score 0.3762473596763519\n",
      "score0 0.2352919211955501\n",
      "score 0.2052696298617911\n",
      "score0 0.131752105915152\n",
      "score 0.12810236544594364\n",
      "score0 0.512513033355675\n",
      "score 0.35803729665498435\n",
      "score0 0.5125108365333167\n",
      "score 0.3580395049587745\n",
      "score0 0.5124329972585691\n",
      "score 0.3580025059162613\n",
      "score0 0.5118085902766499\n",
      "score 0.3577646933032798\n",
      "score0 0.5071076077507666\n",
      "score 0.35562745380819183\n",
      "score0 0.47382249314844277\n",
      "score 0.3389121469157812\n",
      "score0 0.24152144555783636\n",
      "score 0.21352731311977452\n",
      "score0 0.12711732863263825\n",
      "score 0.1466229657810685\n",
      "score0 0.45361805344933376\n",
      "score 0.593578020653763\n",
      "score0 0.45361615153682905\n",
      "score 0.593572578408959\n",
      "score0 0.4535447172991794\n",
      "score 0.593477566247707\n",
      "score0 0.45293426202494597\n",
      "score 0.5925689915596073\n",
      "score0 0.44840881686606465\n",
      "score 0.5860032146887787\n",
      "score0 0.41626906196277436\n",
      "score 0.5399157930379517\n",
      "score0 0.2027740965271606\n",
      "score 0.23472247483016354\n",
      "score0 0.12968938256401402\n",
      "score 0.13347504772097332\n",
      "score0 0.4743426000231224\n",
      "score 0.5106725811809827\n",
      "score0 0.4743403318404859\n",
      "score 0.5107068854712286\n",
      "score0 0.4742623459276792\n",
      "score 0.5105802026797602\n",
      "score0 0.473659156366524\n",
      "score 0.5097440988095568\n",
      "score0 0.46912605618306835\n",
      "score 0.5039363676949021\n",
      "score0 0.43689076967286566\n",
      "score 0.46458602547333405\n",
      "score0 0.21849976059233447\n",
      "score 0.20755096504622414\n",
      "score0 0.13459190990658895\n",
      "score 0.11533255058986029\n",
      "score0 0.4670805255121121\n",
      "score 0.5397379212816443\n",
      "score0 0.46707930262247505\n",
      "score 0.5397361742922379\n",
      "score0 0.4670104264411974\n",
      "score 0.5396528678447698\n",
      "score0 0.4664526636958143\n",
      "score 0.5389666738036154\n",
      "score0 0.46226488870970317\n",
      "score 0.5338173679971253\n",
      "score0 0.4326621047079534\n",
      "score 0.4974669818825027\n",
      "score0 0.22756657728737725\n",
      "score 0.2456024174934284\n",
      "score0 0.13159055653733012\n",
      "score 0.13015922338889777\n",
      "score0 0.5005178718678995\n",
      "score 0.40599723499178353\n",
      "score0 0.5005167546650954\n",
      "score 0.40599633176246\n",
      "score0 0.5004420800888059\n",
      "score 0.4059422164685736\n",
      "score0 0.4998870800259873\n",
      "score 0.40552222913502795\n",
      "score0 0.4956426223706003\n",
      "score 0.4023215971763477\n",
      "score0 0.46559187311060657\n",
      "score 0.37966987317865536\n",
      "score0 0.2537045993324523\n",
      "score 0.2199996756843613\n",
      "score0 0.1325381451619979\n",
      "score 0.12872319939830035\n",
      "score0 0.5125133081857842\n",
      "score 0.35803330480586393\n",
      "score0 0.5125117136895134\n",
      "score 0.35803328964234976\n",
      "score0 0.5124358477720459\n",
      "score 0.35798827525162086\n",
      "score0 0.5118753694166158\n",
      "score 0.3576916114711681\n",
      "score0 0.5076061228506461\n",
      "score 0.35541905631349496\n",
      "score0 0.47731151822202617\n",
      "score 0.33920729109725695\n",
      "score0 0.2625924976565756\n",
      "score 0.22416764349238866\n",
      "score0 0.12787512417941937\n",
      "score 0.14723767327730672\n",
      "score0 0.4536162514811893\n",
      "score 0.5935868547145071\n",
      "score0 0.45361796952602007\n",
      "score 0.593589860135872\n",
      "score0 0.45354194911241286\n",
      "score 0.5934815534224108\n",
      "score0 0.45299099155799616\n",
      "score 0.5926900784851128\n",
      "score0 0.44883344810809184\n",
      "score 0.5867208179304532\n",
      "score0 0.41940296992835074\n",
      "score 0.544548454142373\n",
      "score0 0.2175377640461954\n",
      "score 0.2553357159995721\n",
      "score0 0.13034269846184507\n",
      "score 0.13426791418878226\n",
      "score0 0.47433986847762566\n",
      "score 0.5107024989175574\n",
      "score0 0.4743380135132744\n",
      "score 0.5106989804007306\n",
      "score0 0.4742643226494785\n",
      "score 0.5106156950041214\n",
      "score0 0.4737103194052309\n",
      "score 0.5099532399402306\n",
      "score0 0.46953049307657124\n",
      "score 0.5049755441092347\n",
      "score0 0.43988017088441267\n",
      "score 0.4698192784551337\n",
      "score0 0.23417819582149788\n",
      "score 0.22600739874743\n",
      "score0 0.13534250255360739\n",
      "score 0.11597374697792347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01417414034377762, tolerance: 0.0007238095238095239\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03776632323773832, tolerance: 0.0014142857142857143\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.009946795697898914, tolerance: 0.0016702380952380953\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.005720009908585233, tolerance: 0.0014702380952380954\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010424743243297563, tolerance: 0.0014702380952380954\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00574\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00057\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00006\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 266.47474\n",
      "sigma = 1.00000, lambda = 0.01000, score = -0.16565\n",
      "sigma = 1.00000, lambda = 0.10000, score = -3.04519\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.28275\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.25559\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.02854\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00289\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00029\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00003\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 948.91465\n",
      "sigma = 10.00000, lambda = 0.01000, score = 161.63672\n",
      "sigma = 10.00000, lambda = 0.10000, score = 2.44273\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.54237\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.50450\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.25188\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.03851\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00406\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00041\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 5.02760\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.49903\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50002\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.49937\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.48087\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.26060\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.04206\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00446\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.49971\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = 0.25\n",
      "Approximate alpha-relative KL-divergence = 1.36\n",
      "RuLSIF completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.004462588670622214, tolerance: 0.001414285714285714\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02108526435580993, tolerance: 0.0014702380952380954\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04035849606787334, tolerance: 0.0014142857142857143\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016253900671507893, tolerance: 0.0012952380952380952\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0054836861440712426, tolerance: 0.0013559523809523815\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13100721637634116, tolerance: 0.0014702380952380954\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.000935530584372124, tolerance: 0.000880952380952381\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00666\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00067\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00007\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 461.90540\n",
      "sigma = 1.00000, lambda = 0.01000, score = 6.58337\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.44810\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.14674\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.23472\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.02616\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00265\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00026\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00003\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1904.62401\n",
      "sigma = 10.00000, lambda = 0.01000, score = 142.38934\n",
      "sigma = 10.00000, lambda = 0.10000, score = 0.82809\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.53841\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.50814\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.25601\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.03927\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00414\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00042\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 1.78837\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.50016\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50004\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.49940\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.48091\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.26064\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.04207\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.49971\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49908\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.49975\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.49975\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.49970\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49907\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.48064\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.26068\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.04209\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00447\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00045\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00004\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -0.13\n",
      "Approximate alpha-relative KL-divergence = 1.44\n",
      "RuLSIF completed.\n",
      "[[-0.00924895  0.00131693  0.67213809 -0.04224992  0.         -0.01400762]\n",
      " [-0.03598872  0.92242253  0.02534887 -0.00797322  0.         -0.01509219]\n",
      " [ 0.13062891  0.00131981  0.21934728 -0.00908733  0.         -0.01351759]\n",
      " ...\n",
      " [-0.01863883  0.00131721  0.48253541  0.02620675  0.         -0.00922575]\n",
      " [-0.02212595  0.00131702  0.73539365 -0.0360006   0.         -0.01352904]\n",
      " [ 0.45112927  0.00192716  0.03316698  0.07270269  0.          0.27433368]]\n",
      "[[0.00307018 0.00307018 0.00307018 0.00307018 0.00307018 0.00307018]\n",
      " [1.08067691 1.08067691 1.08067691 1.08067691 1.08067691 1.08067691]\n",
      " [0.00572797 0.00572797 0.00572797 0.00572797 0.00572797 0.00572797]\n",
      " ...\n",
      " [0.01789496 0.01789496 0.01789496 0.01789496 0.01789496 0.01789496]\n",
      " [0.00446897 0.00446897 0.00446897 0.00446897 0.00446897 0.00446897]\n",
      " [2.25456359 2.25456359 2.25456359 2.25456359 2.25456359 2.25456359]]\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6814401102268364\n",
      "score 0.4673928367566565\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6502996478698497\n",
      "score 0.4579343939673723\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5042245770355422\n",
      "score 0.4413488391129712\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4117543853764784\n",
      "score 0.4030721354687641\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.17956905784287885\n",
      "score 0.17863893387990976\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.15157196621786065\n",
      "score 0.15223167862384893\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1494497515843579\n",
      "score 0.150240711664612\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.14924228509841353\n",
      "score 0.15004618783121568\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.638225665460152\n",
      "score 0.7693691292369712\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5932939234266311\n",
      "score 0.7341055832583996\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.43825072780883234\n",
      "score 0.6715941643018104\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3385110566085425\n",
      "score 0.6135694200949706\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.16296323965688694\n",
      "score 0.20999826656819515\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.14503048905878074\n",
      "score 0.17647107926895944\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1435162029536081\n",
      "score 0.17379192874232324\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.14336698430311023\n",
      "score 0.1735291849354481\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5973159055140644\n",
      "score -0.022054414842490377\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5962225642343726\n",
      "score -0.021760739849873367\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5881185090177625\n",
      "score -0.019678801209681296\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5311895786022407\n",
      "score -0.005755562602791087\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.22458712028671166\n",
      "score 0.06464073728518582\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.17229496549401802\n",
      "score 0.07224140651854583\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.16896429799110868\n",
      "score 0.07245309934264581\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.16864280868237533\n",
      "score 0.07247096772656839\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6991012875940152\n",
      "score 0.44404906961350327\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6709409688974155\n",
      "score 0.43381440189218085\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5135402698281618\n",
      "score 0.41992725489670574\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.41776372005194085\n",
      "score 0.38380828743447315\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.18187341791956166\n",
      "score 0.17274393188454432\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.15291435264729153\n",
      "score 0.1470053348146198\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.15074652997477145\n",
      "score 0.14506686839915442\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.15053479624820093\n",
      "score 0.1448774604627344\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5525331766220148\n",
      "score 0.6924286662341939\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5335249740377027\n",
      "score 0.675471309685288\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.45877347703714266\n",
      "score 0.6346248409745425\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.35692315543795694\n",
      "score 0.5788478866754065\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.15814644154252766\n",
      "score 0.24113325980555164\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.13709375152410536\n",
      "score 0.20883351871644915\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.13539843778069993\n",
      "score 0.2063209262902866\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1352319752363832\n",
      "score 0.20607498062575869\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6814369639974305\n",
      "score 0.46738596584781755\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6502882842260671\n",
      "score 0.4579265138890873\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.504221044659632\n",
      "score 0.4413513854837852\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.411754381333694\n",
      "score 0.4030721306040146\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.17956905599002745\n",
      "score 0.1786389320591521\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1515719662187644\n",
      "score 0.152231678622292\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.14944975158464055\n",
      "score 0.15024071166545783\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.14924228509840753\n",
      "score 0.15004618783121018\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6382236397138683\n",
      "score 0.7693706064770522\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5932954739262509\n",
      "score 0.7341056332664118\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4382547633657598\n",
      "score 0.6716006335379417\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3385110502864941\n",
      "score 0.6135694007011853\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1629632395180737\n",
      "score 0.20999826523685278\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1450304891021315\n",
      "score 0.1764710793481446\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1435162029531753\n",
      "score 0.1737919287353425\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1433669843031104\n",
      "score 0.1735291849354487\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5973140021631806\n",
      "score -0.022053970121985206\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5962224611704476\n",
      "score -0.02176072709185367\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5881184353779593\n",
      "score -0.0196788301832119\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5311895802625166\n",
      "score -0.005755566732659367\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.22458711981894489\n",
      "score 0.06464073655950349\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1722949654631289\n",
      "score 0.07224140652449255\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.16896429799077256\n",
      "score 0.07245309934387356\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.16864280868234638\n",
      "score 0.07247096772656995\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-b6118b44a750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_est_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mdm_fit\u001b[0;34m(self, folds, num_basis, sigma_list, lda_list, algorithm, logit)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                         \u001b[0mx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                         \u001b[0ma_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                         \u001b[0my_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[0;32m-> 1016\u001b[0;31m                                           old_fval, old_old_fval, amin=1e-100, amax=1e100)\n\u001b[0m\u001b[1;32m   1017\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Line search failed to find a better solution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m     ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[1;32m    835\u001b[0m                              \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                              **kwargs)\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mextra_condition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe1\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m     99\u001b[0m     stp, fval, old_fval = scalar_search_wolfe1(\n\u001b[1;32m    100\u001b[0m             \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mscalar_search_wolfe1\u001b[0;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0malpha1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mphi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mderphi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mderphi\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mgval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnewargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mgc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mapprox_fprime\u001b[0;34m(xk, f, epsilon, *args)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \"\"\"\n\u001b[0;32m--> 756\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                         \u001b[0mx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                         \u001b[0ma_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mdm_estimator\u001b[0;34m(self, x, a, y, z, f_hst, f_evl, bpol, r, sn, beta, lmd)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdm_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_basis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Ridge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mXC_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTC_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCC_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_basis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_basis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mdm_objective_function\u001b[0;34m(self, x, a, y, z, f_hst, f_evl, bpol, r, sn, beta)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ker_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ker_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ker_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                 \u001b[0mf_hst_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ker_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mf_evl_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ker_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_name = 'satimage'\n",
    "num_trials = 1\n",
    "sample_size = 300\n",
    "\n",
    "if data_name == 'satimage':\n",
    "    data_name = 'satimage.scale'\n",
    "elif data_name == 'vehicle':\n",
    "    data_name = 'vehicle.scale'\n",
    "\n",
    "alphas = [0.7, 0.4, 0.0]\n",
    "\n",
    "tau_list = np.zeros(num_trials)\n",
    "res_ipw3_list = np.zeros((num_trials, len(alphas)))\n",
    "res_dm_list = np.zeros((num_trials, len(alphas)))\n",
    "res_dml2_list = np.zeros((num_trials, len(alphas)))\n",
    "\n",
    "res_ipw3_sn_list = np.zeros((num_trials, len(alphas)))\n",
    "res_dml2_sn_list = np.zeros((num_trials, len(alphas)))\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    X, Y, Y_matrix, train_test_split, classes, N, N_train, N_test = data_generation(data_name, sample_size)\n",
    "\n",
    "    X_train, X_test = X[train_test_split], X[~train_test_split]\n",
    "\n",
    "    Y_matrix_train, Y_matrix_test = Y_matrix[train_test_split], Y_matrix[~train_test_split]\n",
    "\n",
    "    for idx_alpha in  range(len(alphas)):    \n",
    "        alpha = alphas[idx_alpha]\n",
    "\n",
    "        pi_behavior, pi_evaluation  = behavior_and_evaluation_policy(X, Y, train_test_split, classes, alpha=alpha)\n",
    "\n",
    "        pi_behavior_train = pi_behavior[train_test_split]\n",
    "        pi_evaluation_train, pi_evaluation_test = pi_evaluation[train_test_split], pi_evaluation[~train_test_split]\n",
    "\n",
    "        tau = true_value(Y_matrix_test, pi_evaluation_test, N_test)\n",
    "        tau_list[trial] = tau\n",
    "\n",
    "        perm = np.random.permutation(N_train)\n",
    "        X_seq_train, Y_matrix_seq_train, pi_behavior_seq_train, pi_evaluation_seq_train = X_train[perm], Y_matrix_train[perm], pi_behavior_train[perm], pi_evaluation_train[perm]\n",
    "\n",
    "        Y_historical_matrix = np.zeros(shape=(N_train, len(classes)))\n",
    "        A_historical_matrix = np.zeros(shape=(N_train, len(classes)))\n",
    "\n",
    "        for i in range(N_train):\n",
    "            a = np.random.choice(classes, p=pi_behavior_seq_train[i])\n",
    "            Y_historical_matrix[i, a] = Y_matrix_seq_train[i, a]\n",
    "            A_historical_matrix[i, a] = 1\n",
    "            \n",
    "        estimators = op_learning(X_seq_train, A_historical_matrix, Y_historical_matrix, X_test, classes, pi_evaluation_seq_train, pi_evaluation_test)\n",
    "        \n",
    "        estimators.ipw_est_parameters()\n",
    "        epol_ipw = estimators.ipw_fit(folds=5, algorithm='Ridge', self_norm=False)\n",
    "        epol_ipw_sn = estimators.ipw_fit(folds=5, algorithm='Ridge', self_norm=True)\n",
    "\n",
    "        estimators.dm_est_parameters()\n",
    "        epol_dm = estimators.dm_fit(folds=5, algorithm='Ridge', self_norm=False)\n",
    "        \n",
    "        estimators.dml_est_parameters(folds=5, algorithm='Ridge')\n",
    "        epol_dml = estimators.dml_fit(folds=5, algorithm='Ridge', self_norm=False)\n",
    "        epol_dml_sn = estimators.dml_fit(folds=5, algorithm='Ridge', self_norm=True)\n",
    "\n",
    "        res_ipw3 =  true_value(Y_matrix_test, epol_ipw, N_test)\n",
    "        res_ipw3_sn =  true_value(Y_matrix_test, epol_ipw_sn, N_test)\n",
    "        res_dm =  true_value(Y_matrix_test, epol_dm, N_test)\n",
    "        res_dml2 =  true_value(Y_matrix_test, epol_dml, N_test)\n",
    "        res_dml2_sn =  true_value(Y_matrix_test, epol_dml_sn, N_test)\n",
    "\n",
    "        print('trial', trial)\n",
    "        print('True:', tau)\n",
    "        print('IPW3:', res_ipw3)\n",
    "        print('IPW3_SN:', res_ipw3_sn)\n",
    "        print('DM:', res_dm)\n",
    "        print('DML2:', res_dml2)\n",
    "        print('DML2_SN:', res_dml2_sn)\n",
    "\n",
    "        res_ipw3_list[trial, idx_alpha] = res_ipw3\n",
    "        res_ipw3_sn_list[trial, idx_alpha] = res_ipw3_sn\n",
    "        res_dm_list[trial, idx_alpha] = res_dm\n",
    "        res_dml2_list[trial, idx_alpha] = res_dml2\n",
    "        res_dml2_sn_list[trial, idx_alpha] = res_dml2_sn\n",
    "\n",
    "        np.savetxt(\"exp_results/true_value_%s.csv\"%data_name, tau_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_opl_ipw3_%s.csv\"%data_name, res_ipw3_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_opl_ipw3_sn_%s.csv\"%data_name, res_ipw3_sn_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_opl_dm_%s.csv\"%data_name, res_dm_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_opl_dml2_%s.csv\"%data_name, res_dml2_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_opl_dml2_sn_%s.csv\"%data_name, res_dml２_sn_list, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = op_learning(X_seq_train, A_historical_matrix, Y_historical_matrix, X_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py:224: RuntimeWarning: invalid value encountered in true_divide\n",
      "  G = G_numer / G_denom\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py:236: RuntimeWarning: invalid value encountered in true_divide\n",
      "  B_x = d_mx / f_x - G * d_fx / f_x\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py:237: RuntimeWarning: invalid value encountered in true_divide\n",
      "  B_x = (G_numer * d_fx - G_denom * d_mx) / (G_denom**2)\n",
      "../cs_opl.py:51: RuntimeWarning: invalid value encountered in less\n",
      "  mu[mu < 0.001] = 0.001\n",
      "../cs_opl.py:52: RuntimeWarning: invalid value encountered in greater\n",
      "  mu[mu > 0.999] = 0.999\n",
      "../cs_opl.py:54: RuntimeWarning: invalid value encountered in greater\n",
      "  if len(mu[~((mu > -100)&(mu < 100))]) == 0:\n",
      "../cs_opl.py:54: RuntimeWarning: invalid value encountered in less\n",
      "  if len(mu[~((mu > -100)&(mu < 100))]) == 0:\n"
     ]
    }
   ],
   "source": [
    "estimators.ipw_est_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-5fb2cbd263bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdml_est_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdml_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mdml_est_parameters\u001b[0;34m(self, folds, method)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ker_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ker_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ker_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mf_hst_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ker_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/kernel_regression.py\u001b[0m in \u001b[0;36mKernelRegression\u001b[0;34m(x_train, t_train, x_test, folds, num_basis, sigma_list, lda_list, algorithm, logit)\u001b[0m\n\u001b[1;32m     82\u001b[0m                         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    710\u001b[0m             _pre_fit(X, y, None, self.precompute, self.normalize,\n\u001b[1;32m    711\u001b[0m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshould_copy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                      check_input=check_input)\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_pre_fit\u001b[0;34m(X, y, Xy, precompute, normalize, fit_intercept, copy, check_input)\u001b[0m\n\u001b[1;32m    525\u001b[0m         X, y, X_offset, y_offset, X_scale = _preprocess_data(\n\u001b[1;32m    526\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             check_input=check_input)\n\u001b[0m\u001b[1;32m    528\u001b[0m     if hasattr(precompute, '__array__') and (\n\u001b[1;32m    529\u001b[0m             \u001b[0mfit_intercept\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(X, y, fit_intercept, normalize, copy, sample_weight, return_mean, check_input)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],\n\u001b[0;32m--> 126\u001b[0;31m                         dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2182\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = estimators.dml_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n",
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n",
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 nan\n",
      "score nan\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n",
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 nan\n",
      "score nan\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 44.80854296996513\n",
      "score 1.308881683828798\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 43.75970334024666\n",
      "score 1.281926501730938\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 37.000672326075744\n",
      "score 1.3933336843048583\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 32.035081210935836\n",
      "score 1.1549892784706297\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n",
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 nan\n",
      "score nan\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n",
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 nan\n",
      "score nan\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n",
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 nan\n",
      "score nan\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 43.38201695690485\n",
      "score 0.028327274226219345\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 42.64913940074736\n",
      "score 0.02774498698525023\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 37.50440384881384\n",
      "score 0.02449441068755044\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n",
      "../cs_opl.py:535: RuntimeWarning: overflow encountered in exp\n",
      "  epol_hst = np.exp(-np.dot(x, beta))\n",
      "../cs_opl.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
      "  epol_hst = (epol_hst.T/np.sum(epol_hst, axis=1)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 nan\n",
      "score nan\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-4621b0864d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipw_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Ridge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mipw_fit\u001b[0;34m(self, folds, num_basis, sigma_list, lda_list, algorithm, self_norm)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipw_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_bhv_hst_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hst_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mscore0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipw_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_bhv_hst_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hst_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[0;32m-> 1016\u001b[0;31m                                           old_fval, old_old_fval, amin=1e-100, amax=1e100)\n\u001b[0m\u001b[1;32m   1017\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Line search failed to find a better solution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m     ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[1;32m    835\u001b[0m                              \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                              **kwargs)\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mextra_condition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe1\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m     99\u001b[0m     stp, fval, old_fval = scalar_search_wolfe1(\n\u001b[1;32m    100\u001b[0m             \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mscalar_search_wolfe1\u001b[0;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0malpha1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mphi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mderphi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mderphi\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mgval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnewargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mgc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mapprox_fprime\u001b[0;34m(xk, f, epsilon, *args)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \"\"\"\n\u001b[0;32m--> 756\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_hst_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipw_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_bhv_hst_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hst_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mipw_estimator\u001b[0;34m(self, x, a, y, bpol, r, beta, lmd, self_norm)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mbeta_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipw_objective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_norm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCalcDistanceSquared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mipw_objective_function\u001b[0;34m(self, x, a, y, bpol, r, beta, self_norm)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0msn_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbpol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0msn_matrix\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepol_hst\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbpol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = estimators.ipw_fit(folds=5, algorithm='Ridge', self_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators.ipw_fit(folds=5, algorithm='Ridge', self_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b= np.where(estimators.A == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285714285714286"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(b == np.argmax(estimators.bpol_array, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "       False, False,  True,  True,  True,  True, False,  True,  True,\n",
       "       False,  True,  True, False,  True,  True, False,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True,  True, False, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
       "       False,  True,  True])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b == np.argmax(estimators.bpol_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5920741597046222\n",
      "score 0.22304838279334344\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5484855666994175\n",
      "score 0.21996685751965783\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4370257580785539\n",
      "score 0.21936581372995897\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.38258719055318824\n",
      "score 0.1960691619150928\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1453535381565161\n",
      "score 0.0998240228390467\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1176025078946443\n",
      "score 0.08761088145672416\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11558404389026423\n",
      "score 0.08667139715272348\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11538739056311095\n",
      "score 0.08657940390012661\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5282926747853551\n",
      "score 0.34780252844829007\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5109027912907325\n",
      "score 0.3399568409582776\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4055612035494752\n",
      "score 0.34152450024979086\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3520824636363296\n",
      "score 0.29516334917349535\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.13652752334001284\n",
      "score 0.1218531906687522\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11333915459848752\n",
      "score 0.10401154383905852\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11156972327647918\n",
      "score 0.10266737187124189\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11139669598380719\n",
      "score 0.10253608878740317\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4669952640284015\n",
      "score 0.5598376835945742\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4536444020528354\n",
      "score 0.5479374514611632\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39621833184501654\n",
      "score 0.5230302456006007\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.30080517793350625\n",
      "score 0.445454783624878\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12596436730438332\n",
      "score 0.14048539803777188\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11013069838605852\n",
      "score 0.11546856899192826\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10883319245221607\n",
      "score 0.11348199984211557\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10870562913990972\n",
      "score 0.11328726563460262\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39710965976676127\n",
      "score 0.41805212964237726\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.396079961879122\n",
      "score 0.41692519179420096\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3884656198223958\n",
      "score 0.40900859533790435\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.334965868974984\n",
      "score 0.35655848867760337\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12478987502715468\n",
      "score 0.16292053667643655\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10340640241501377\n",
      "score 0.14324963682602848\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10178968585387801\n",
      "score 0.14173865512417533\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10163179661348384\n",
      "score 0.14159080385851133\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4899802836787689\n",
      "score 0.45161649531972986\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.47892580349655745\n",
      "score 0.4444898846337131\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.37994757565574855\n",
      "score 0.4400251872869799\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.32715191426173335\n",
      "score 0.36969982092619424\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.13187700208147127\n",
      "score 0.1282170554277957\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11266670154018488\n",
      "score 0.10600801054703018\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11114808929229211\n",
      "score 0.10428776697300235\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11099921460148299\n",
      "score 0.1041194306195582\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5635881019506099\n",
      "score 0.22286923224322044\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5484869187349841\n",
      "score 0.21996710336532868\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4370258072725079\n",
      "score 0.219365849262182\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3825871755589518\n",
      "score 0.19606915506087352\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.14535353802981782\n",
      "score 0.09982402276939147\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11760250789924143\n",
      "score 0.08761088145854126\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11558404388998775\n",
      "score 0.08667139715271259\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11538739056310993\n",
      "score 0.0865794039001261\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5282809984376247\n",
      "score 0.3477919384994353\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5109036637770326\n",
      "score 0.33995855174709066\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.40556122120712096\n",
      "score 0.34152458108981243\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3520824622860344\n",
      "score 0.2951633486948974\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1365275222677335\n",
      "score 0.1218531896682678\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11333915460704265\n",
      "score 0.10401154384663627\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11156972327643608\n",
      "score 0.10266737187029795\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11139669598380705\n",
      "score 0.10253608878740306\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.46696538154341555\n",
      "score 0.559794474152187\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4536457173956152\n",
      "score 0.5479384461546917\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3962223970781258\n",
      "score 0.5230247834768086\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3008051897008194\n",
      "score 0.44545480619077094\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12596436741675351\n",
      "score 0.14048539847176716\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1101306983712559\n",
      "score 0.11546856896645005\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10883319245208137\n",
      "score 0.11348199984285143\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10870562913990978\n",
      "score 0.11328726563460267\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39710824096702735\n",
      "score 0.4180504537604077\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3960800146867093\n",
      "score 0.4169253038803904\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.388465616795696\n",
      "score 0.40900859508747417\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3349658604597687\n",
      "score 0.3565584809798905\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12478987478318442\n",
      "score 0.16292053670435672\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10340640242410976\n",
      "score 0.14324963683618555\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10178968585409946\n",
      "score 0.14173865512648504\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1016317966134826\n",
      "score 0.1415908038585102\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4946383433230932\n",
      "score 0.45122974290138096\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4789275544882864\n",
      "score 0.4444946104234012\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3799475939961945\n",
      "score 0.4400252578315883\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3271519173717991\n",
      "score 0.3696998236786286\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.131877003427078\n",
      "score 0.12821705704179281\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11266670156609912\n",
      "score 0.10600801057069073\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11114808929269412\n",
      "score 0.10428776697469035\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11099921460148598\n",
      "score 0.10411943061956155\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5635982295000443\n",
      "score 0.22287777254382612\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5484878644792979\n",
      "score 0.21998054176646475\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.43702584164451586\n",
      "score 0.21936695670879885\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.38258737707378765\n",
      "score 0.19607290509281383\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1453536713218645\n",
      "score 0.099824667320724\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11760251441497964\n",
      "score 0.0876109211536627\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11558404450197507\n",
      "score 0.08667140096090784\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11538739062390815\n",
      "score 0.08657940427927294\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5310106895740123\n",
      "score 0.3473594774736861\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5109118626178322\n",
      "score 0.33996859318091605\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4055612882894826\n",
      "score 0.3415252415139029\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3520828735514301\n",
      "score 0.29516560914903966\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1365277588390545\n",
      "score 0.12185366049257144\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11333916689839937\n",
      "score 0.10401157212752943\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1115697244381343\n",
      "score 0.10266737457678234\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11139669609931088\n",
      "score 0.10253608905671972\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.46698491688187\n",
      "score 0.5598198770992958\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4536477457813831\n",
      "score 0.5479462775800872\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3962386302128463\n",
      "score 0.5229932960838942\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.30080610632406224\n",
      "score 0.44545371285064717\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12596467536463946\n",
      "score 0.1404856766818791\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11013071664944213\n",
      "score 0.11546858255176434\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1088331941939667\n",
      "score 0.11348200110801976\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10870562931330882\n",
      "score 0.11328726576033098\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39710953797442494\n",
      "score 0.418051956201086\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3960800387704337\n",
      "score 0.4169252563525666\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3884657216433764\n",
      "score 0.40900849878452866\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3349665420985158\n",
      "score 0.3565585293300621\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12479024835820463\n",
      "score 0.16292079512189822\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 0.10340642246564873\n",
      "score 0.1432496496713782\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10178968775375105\n",
      "score 0.14173865633599808\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10163179680237974\n",
      "score 0.14159080397879392\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.48996644681547585\n",
      "score 0.45161110390632925\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.478927804877456\n",
      "score 0.444495064050424\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.37994781838080577\n",
      "score 0.4400237766512656\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.32715293594272604\n",
      "score 0.36969767079655697\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.13187744350696867\n",
      "score 0.12821720377353985\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11266672617861144\n",
      "score 0.10600801396521792\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11114809163040432\n",
      "score 0.10428776725216349\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11099921483412137\n",
      "score 0.10411943064699715\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6604764638334212\n",
      "score 0.48424140171646374\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6545312410188002\n",
      "score 0.4802743364973444\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6432572134709669\n",
      "score 0.4553916412699473\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5720307817502396\n",
      "score 0.36846979718593825\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39632821402515084\n",
      "score 0.2228827179706662\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.15208360053032002\n",
      "score 0.1034283060941818\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11777723866084236\n",
      "score 0.08768794626263246\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11559872634275015\n",
      "score 0.08667744672984075\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6521647870042709\n",
      "score 0.5298101442600237\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6584030375277399\n",
      "score 0.5256069021794467\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.631920514607672\n",
      "score 0.509409164135575\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5562624971160469\n",
      "score 0.4341072920305783\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3723337219602136\n",
      "score 0.32177157585009214\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.14054223318393333\n",
      "score 0.12794095521086146\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11340980185279476\n",
      "score 0.10424734985479642\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11157480479015464\n",
      "score 0.10268867711626763\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.60308213523413\n",
      "score 0.7119284475460775\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.627238836817692\n",
      "score 0.6354554903963243\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6121967930629009\n",
      "score 0.6166405950199815\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5244951856970984\n",
      "score 0.5812148395757716\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.32811929490202574\n",
      "score 0.4544578522847884\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12948161653166407\n",
      "score 0.1463495390899236\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11022669966862203\n",
      "score 0.11566528438771356\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10884115175022938\n",
      "score 0.11349921932183027\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6050441439090781\n",
      "score 0.7095533515644383\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5978793539240151\n",
      "score 0.7097552052235985\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5789309703860128\n",
      "score 0.6925982553757182\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5112681505438188\n",
      "score 0.5910690355155377\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3537924427224405\n",
      "score 0.36503809291444\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.13082287435537143\n",
      "score 0.16726114597699593\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10359251244379394\n",
      "score 0.14335438044430576\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10180589213570856\n",
      "score 0.1417471874143796\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.6186293411814352\n",
      "score 0.6506825945076529\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.62746660959685\n",
      "score 0.6440974230483846\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.5981345191710622\n",
      "score 0.6398384266493631\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.520521968912627\n",
      "score 0.5943689303755851\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.34250204608529244\n",
      "score 0.39764352960362565\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1355553048227954\n",
      "score 0.1345567775803423\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11272765491184206\n",
      "score 0.10625751233823011\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11115236178219014\n",
      "score 0.1043103969185408\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4459515876960882\n",
      "score 0.22348401747346885\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.44594527859402683\n",
      "score 0.2234827942284373\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4458708196120769\n",
      "score 0.22344558362021355\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.44527530993072006\n",
      "score 0.22318498533221193\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4407642121640299\n",
      "score 0.2212269647976003\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4088803522222431\n",
      "score 0.2076685940996873\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1954168279068552\n",
      "score 0.120538772040557\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11958584882255624\n",
      "score 0.0885382604150544\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4142989555712928\n",
      "score 0.35007931061333775\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.41429570562861484\n",
      "score 0.3500771553963038\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4142203102139888\n",
      "score 0.3500192618030486\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4136302645771255\n",
      "score 0.34954775072547234\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.40916257965684066\n",
      "score 0.345933251200116\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3776448240823416\n",
      "score 0.3201742864718139\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1748355586581003\n",
      "score 0.153618837032404\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11503833255468475\n",
      "score 0.10537651236807469\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.36121451887910505\n",
      "score 0.562425117829142\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.36120507036842675\n",
      "score 0.5624043642370374\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.36113065078619727\n",
      "score 0.5622766837743958\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.36055009507430763\n",
      "score 0.5612415112391974\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.35620827049118237\n",
      "score 0.5535118371437495\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.32556649555166123\n",
      "score 0.498864220048551\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1484349182107766\n",
      "score 0.18114016961047885\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11132203524790833\n",
      "score 0.11752088447592096\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3972617473928495\n",
      "score 0.41823795924147267\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39725853063229044\n",
      "score 0.4182341670137236\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3971770082033895\n",
      "score 0.4181649561739926\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39658745800689077\n",
      "score 0.41761531937120444\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3921427505122028\n",
      "score 0.4135132745971028\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.360736453305848\n",
      "score 0.3844477856886135\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.16073716830159096\n",
      "score 0.19748016904549234\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.1049860601906828\n",
      "score 0.14482991263209935\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.38855536744666963\n",
      "score 0.4530508480676434\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3885516641172383\n",
      "score 0.4530445530373779\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3884714688440526\n",
      "score 0.452946916927592\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3878885813847545\n",
      "score 0.4521898416241415\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3834915561143242\n",
      "score 0.4465750066536122\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.35244362923662215\n",
      "score 0.40722977720253545\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.16155054287259685\n",
      "score 0.16624621751940516\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11408529204966003\n",
      "score 0.10779612498849132\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "(210, 42)\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4459500660205502\n",
      "score 0.22348339386688704\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4459500660205502\n",
      "score 0.22348339386688704\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.44587721751727183\n",
      "score 0.22345065156139554\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.445329426704208\n",
      "score 0.22322943483114166\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.44115422356409023\n",
      "score 0.2215292764416083\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4116657937154309\n",
      "score 0.20949665295816633\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.20866364333583481\n",
      "score 0.12667374293843434\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.12006716397870171\n",
      "score 0.08881733861023508\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4143017881624785\n",
      "score 0.3500792395161815\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.4142995073774448\n",
      "score 0.35007730153798255\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.41422576496765506\n",
      "score 0.3500162629854937\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.41368567938174156\n",
      "score 0.3495738174857664\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.40956402637678346\n",
      "score 0.34619386232578353\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.38049732462069985\n",
      "score 0.32236819820560486\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.18584526113328043\n",
      "score 0.16282931214961124\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11545797450994937\n",
      "score 0.10574666990597156\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3612114738159191\n",
      "score 0.5624196562326478\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.36120808790352765\n",
      "score 0.5624126859599318\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.36113722331807785\n",
      "score 0.5622883956608856\n",
      "(168, 6)\n",
      "(211, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score0 0.3606097410062051\n",
      "score 0.5613375391952614\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.35659671394840203\n",
      "score 0.5541195129032266\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3283484932769597\n",
      "score 0.5033856026279437\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.15523084591509753\n",
      "score 0.19276698860819366\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11164577519658936\n",
      "score 0.11801345535546867\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3972607356195228\n",
      "score 0.41823785360512017\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3972585610509419\n",
      "score 0.4182355436084012\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3971889028874397\n",
      "score 0.4181748775881799\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.396648034442802\n",
      "score 0.41766552262096823\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.39253841105612397\n",
      "score 0.4138270728455681\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.363568491197403\n",
      "score 0.3867385713965819\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.17131226422761484\n",
      "score 0.20694876361720982\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.10540771933408649\n",
      "score 0.14519309125381058\n",
      "5\n",
      "4\n",
      "1\n",
      "(168, 210)\n",
      "(168, 211)\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3885570455560412\n",
      "score 0.45305581540022677\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3885540002848692\n",
      "score 0.4530514078120117\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.38849141184368774\n",
      "score 0.45297370322013086\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3879489683249601\n",
      "score 0.45228590185200923\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.3838816851017538\n",
      "score 0.44713469653452587\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.35520773797041594\n",
      "score 0.4108593078346915\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.17030289959196115\n",
      "score 0.177030961428772\n",
      "(168, 6)\n",
      "(211, 6)\n",
      "score0 0.11446657928615778\n",
      "score 0.10821946243489258\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'linear_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-0c716661f9d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/cs_ope/cs_ope/cs_opl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, folds, num_basis, sigma_list, lda_list, algorithm, logit)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_chosen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saga'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                 \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_chosen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_model' is not defined"
     ]
    }
   ],
   "source": [
    "estimators.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(pi_behavior_seq_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Y_matrix, train_test_split, x_prob, classes, N, N_train, N_test = data_generation(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelRidge(alpha=0.01, coef0=1, degree=3, gamma=None, kernel='linear',\n",
       "            kernel_params=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KernelRidge(alpha=0.01)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.LogisticRegression(penalty='l1', C=0.1, solver='saga', multi_class='multi_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 1, ..., 5, 1, 7])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.where(Y_matrix == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 1, ..., 5, 1, 7])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 1, ..., 5, 1, 7])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 159660 into shape (4435,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ffa7af881c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kernel_regression.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, var_type, reg_type, bw, defaults)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_adjust_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_adjust_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/_kernel_base.py\u001b[0m in \u001b[0;36m_adjust_shape\u001b[0;34m(dat, k_vars)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# ndim >1 so many obs many vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 159660 into shape (4435,1)"
     ]
    }
   ],
   "source": [
    "model = KernelReg([1,100,1], X, var_type='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'333'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3'*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cccccccccccccccccccccccccccccccccccc'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'c'*X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_u = sm.nonparametric.KDEMultivariate(data=X, var_type='c'*dim, bw='normal_reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.56532044)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dens_u.pdf(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = dens_u.bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[:50]\n",
    "X = X[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_u = sm.nonparametric.KDEMultivariate(data=X, var_type='c'*X.shape[1], bw='normal_reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelReg(Y, X, var_type='c'*X.shape[1], reg_type='lc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19174831, 0.28450028, 0.27009914, 0.15901443, 0.30847048,\n",
       "       0.28977343, 0.17193598, 0.03891559, 0.20149673, 0.31748389,\n",
       "       0.28018324, 0.28895554, 0.15374873, 0.21203015, 0.57902868,\n",
       "       0.05410153])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, x = model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 16)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119904"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.73930242, 4.83120944, 4.76830902, 4.05162109, 4.54722321,\n",
       "        4.90765989, 4.45411434, 4.89109568, 4.71458883, 4.78442258,\n",
       "        4.7987667 , 4.75148552, 4.46570638, 4.97899378, 4.98634629,\n",
       "        4.81090934]),\n",
       " array([[-2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15,\n",
       "         -2.42074379e+15, -2.42074379e+15, -2.42074379e+15],\n",
       "        [-1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16,\n",
       "         -1.80097159e+16, -1.80097159e+16, -1.80097159e+16],\n",
       "        [-1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16,\n",
       "         -1.01969841e+16, -1.01969841e+16, -1.01969841e+16],\n",
       "        [-1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16,\n",
       "         -1.85016927e+16, -1.85016927e+16, -1.85016927e+16],\n",
       "        [-1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16,\n",
       "         -1.08467117e+16, -1.08467117e+16, -1.08467117e+16],\n",
       "        [-1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16,\n",
       "         -1.14534011e+16, -1.14534011e+16, -1.14534011e+16],\n",
       "        [-4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15,\n",
       "         -4.58262557e+15, -4.58262557e+15, -4.58262557e+15],\n",
       "        [-6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15,\n",
       "         -6.84514435e+15, -6.84514435e+15, -6.84514435e+15],\n",
       "        [ 1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16,\n",
       "          1.56924872e+16,  1.56924872e+16,  1.56924872e+16],\n",
       "        [-9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14,\n",
       "         -9.31466971e+14, -9.31466971e+14, -9.31466971e+14],\n",
       "        [-2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16,\n",
       "         -2.11527895e+16, -2.11527895e+16, -2.11527895e+16],\n",
       "        [-1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16,\n",
       "         -1.96603423e+16, -1.96603423e+16, -1.96603423e+16],\n",
       "        [-2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16,\n",
       "         -2.28852801e+16, -2.28852801e+16, -2.28852801e+16],\n",
       "        [-2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15,\n",
       "         -2.31623367e+15, -2.31623367e+15, -2.31623367e+15],\n",
       "        [-4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15,\n",
       "         -4.30702131e+15, -4.30702131e+15, -4.30702131e+15],\n",
       "        [-1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15,\n",
       "         -1.51003302e+15, -1.51003302e+15, -1.51003302e+15]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[-16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4435, 36)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.]), array([[-99.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2857443  0.1952922  0.26814298 0.07805563 0.05606263 0.11670226]\n",
      " [0.29536795 0.15428734 0.27987302 0.0802635  0.05640643 0.13380176]\n",
      " [0.37533892 0.1459109  0.18370155 0.12563121 0.06332002 0.10609741]\n",
      " ...\n",
      " [0.20679231 0.16011928 0.29419599 0.0838981  0.1119685  0.14302582]\n",
      " [0.31595816 0.13683452 0.25687274 0.10179922 0.11506038 0.07347497]\n",
      " [0.28837575 0.1668954  0.27622361 0.06731022 0.08418668 0.11700835]]\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08136092229213432, tolerance: 0.04414618372652263\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08136092229213432, tolerance: 0.04414618372652263\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09048368117350947, tolerance: 0.04430395631018423\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09048368117350947, tolerance: 0.04430395631018423\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08161668621410456, tolerance: 0.04447331896825037\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08161668621410456, tolerance: 0.04447331896825037\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00056\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00006\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 31.56571\n",
      "sigma = 1.00000, lambda = 0.01000, score = -1.17952\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.35085\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.38121\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.45369\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.06027\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00623\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00062\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 2105.09781\n",
      "sigma = 10.00000, lambda = 0.01000, score = 410.15885\n",
      "sigma = 10.00000, lambda = 0.10000, score = 10.66124\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.53089\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52516\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37793\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08145\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00911\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 20.09672\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.46082\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50031\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50023\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49609\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37495\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08672\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49995\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -1.47\n",
      "Approximate alpha-relative KL-divergence = 1.38\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00073\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00007\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 33.96512\n",
      "sigma = 1.00000, lambda = 0.01000, score = -1.36617\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.53581\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.51109\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.47588\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.06146\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00633\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00063\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1656.12344\n",
      "sigma = 10.00000, lambda = 0.01000, score = 390.18081\n",
      "sigma = 10.00000, lambda = 0.10000, score = 10.43140\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.52032\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52560\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37866\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08175\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00914\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00093\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 18.40631\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.45219\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50032\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50023\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49609\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37495\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08672\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49995\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -1.46\n",
      "Approximate alpha-relative KL-divergence = 1.39\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03031991178854554, tolerance: 0.02230264291344382\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03031991178854554, tolerance: 0.02230264291344382\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.043619514194688236, tolerance: 0.022751581831235764\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.043619514194688236, tolerance: 0.022751581831235764\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00072\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00007\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 28.91635\n",
      "sigma = 1.00000, lambda = 0.01000, score = -0.78712\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.22179\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.30595\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.42709\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.05663\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00585\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00059\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1701.65429\n",
      "sigma = 10.00000, lambda = 0.01000, score = 430.16227\n",
      "sigma = 10.00000, lambda = 0.10000, score = 14.54468\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.50482\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52391\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37739\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08152\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00912\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 28.82476\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.42002\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50030\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50020\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49601\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37485\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08676\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -0.95\n",
      "Approximate alpha-relative KL-divergence = 1.31\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0439595261181438, tolerance: 0.02206473411299383\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0439595261181438, tolerance: 0.02206473411299383\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03466387270604798, tolerance: 0.022250593846881925\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03466387270604798, tolerance: 0.022250593846881925\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04257241943840029, tolerance: 0.02164541446099271\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04257241943840029, tolerance: 0.02164541446099271\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02999003355433416, tolerance: 0.021463531141462978\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02999003355433416, tolerance: 0.021463531141462978\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029693701291321872, tolerance: 0.02147754261963311\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029693701291321872, tolerance: 0.02147754261963311\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00108\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00011\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 59.50772\n",
      "sigma = 1.00000, lambda = 0.01000, score = -2.21544\n",
      "sigma = 1.00000, lambda = 0.10000, score = -3.19060\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.54425\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.43621\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.05483\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00563\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00056\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1845.37172\n",
      "sigma = 10.00000, lambda = 0.01000, score = 362.72388\n",
      "sigma = 10.00000, lambda = 0.10000, score = 12.63708\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.50617\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52621\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37752\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08130\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00909\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 23.92714\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.42282\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50033\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50023\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49603\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37485\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08676\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -0.86\n",
      "Approximate alpha-relative KL-divergence = 1.42\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00102\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00010\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 48.74342\n",
      "sigma = 1.00000, lambda = 0.01000, score = 1.12235\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.43478\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.49822\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.47382\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.06167\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00636\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00064\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1999.37845\n",
      "sigma = 10.00000, lambda = 0.01000, score = 430.62733\n",
      "sigma = 10.00000, lambda = 0.10000, score = 12.26423\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.52086\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52619\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37945\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08199\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00917\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00093\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 21.90755\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.44597\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50032\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50023\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49604\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37487\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08672\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -1.33\n",
      "Approximate alpha-relative KL-divergence = 1.38\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00078\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00008\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 46.87919\n",
      "sigma = 1.00000, lambda = 0.01000, score = -2.06557\n",
      "sigma = 1.00000, lambda = 0.10000, score = -3.14872\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.50040\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.42257\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.05335\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00548\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00055\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00005\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 2182.46624\n",
      "sigma = 10.00000, lambda = 0.01000, score = 422.24404\n",
      "sigma = 10.00000, lambda = 0.10000, score = 11.09138\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.52940\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52442\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37777\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08164\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00913\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 20.36423\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.45932\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50031\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50022\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49602\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37485\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -0.61\n",
      "Approximate alpha-relative KL-divergence = 1.36\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00149\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00015\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 62.54142\n",
      "sigma = 1.00000, lambda = 0.01000, score = 1.22332\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.16187\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.36914\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.47048\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.06229\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00643\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00065\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 10.00000, lambda = 0.00100, score = 1963.16851\n",
      "sigma = 10.00000, lambda = 0.01000, score = 365.56594\n",
      "sigma = 10.00000, lambda = 0.10000, score = 8.20643\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.52881\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52418\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37816\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08173\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00914\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00093\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 14.14147\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.47064\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50030\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50022\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49602\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37485\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -1.28\n",
      "Approximate alpha-relative KL-divergence = 1.29\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.039658722675653735, tolerance: 0.022165703298471957\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.039658722675653735, tolerance: 0.022165703298471957\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.054870372145039426, tolerance: 0.02237794960418357\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.054870372145039426, tolerance: 0.02237794960418357\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03902818322756474, tolerance: 0.022544226827337432\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03902818322756474, tolerance: 0.022544226827337432\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.045200808209827414, tolerance: 0.022670570996965463\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.045200808209827414, tolerance: 0.022670570996965463\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00171\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00017\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00002\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 38.73322\n",
      "sigma = 1.00000, lambda = 0.01000, score = 0.12444\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.39572\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.43401\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.45452\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.05947\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00613\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00062\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1964.32896\n",
      "sigma = 10.00000, lambda = 0.01000, score = 405.38895\n",
      "sigma = 10.00000, lambda = 0.10000, score = 9.64578\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.52058\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52197\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37624\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08127\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00909\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 16.28299\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.46219\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50028\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50019\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49600\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37483\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08676\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -1.78\n",
      "Approximate alpha-relative KL-divergence = 1.32\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00112\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00011\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 53.07018\n",
      "sigma = 1.00000, lambda = 0.01000, score = -1.44841\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.55276\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.38294\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.42864\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.05575\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00575\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00058\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1509.89988\n",
      "sigma = 10.00000, lambda = 0.01000, score = 398.00889\n",
      "sigma = 10.00000, lambda = 0.10000, score = 12.57141\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.51076\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52433\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37657\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08112\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00907\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 23.10916\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.43039\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50031\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50021\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49602\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37484\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08676\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -1.00\n",
      "Approximate alpha-relative KL-divergence = 1.30\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00116\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00012\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 50.76982\n",
      "sigma = 1.00000, lambda = 0.01000, score = -0.50875\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.48143\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.45807\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.43598\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.05504\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00565\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00057\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00006\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1788.29679\n",
      "sigma = 10.00000, lambda = 0.01000, score = 340.14796\n",
      "sigma = 10.00000, lambda = 0.10000, score = 11.03595\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.51514\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52476\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37761\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08142\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00910\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 20.66439\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.44420\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50031\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50021\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49602\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37485\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49576\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49575\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37481\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -1.28\n",
      "Approximate alpha-relative KL-divergence = 1.42\n",
      "RuLSIF completed.\n",
      "0.0020615203965287134\n",
      "0.13496492396029341\n",
      "0.14052462440146124\n",
      "0.14454640025621163\n",
      "0.07316521953909505\n",
      "0.06772742895011623\n",
      "[[0.24102912 0.06764163 0.17187361 0.10285628 0.10133621 0.31526315]\n",
      " [0.22805161 0.16242707 0.24136784 0.13808953 0.09609416 0.1339698 ]\n",
      " [0.4419999  0.12967623 0.16618194 0.09248787 0.09328026 0.07637381]\n",
      " ...\n",
      " [0.22448224 0.16734621 0.23944092 0.13555183 0.10691164 0.12626715]\n",
      " [0.19431824 0.25503681 0.19142227 0.14680791 0.12565408 0.08676068]\n",
      " [0.22133478 0.12516184 0.21841313 0.15798502 0.12293443 0.15417081]]\n",
      "sigma:  0.001\n",
      "lda:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07433223137628175, tolerance: 0.040998332075193954\n",
      "  positive)\n",
      "/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07433223137628175, tolerance: 0.040998332075193954\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00051\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00005\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00001\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 23.52820\n",
      "sigma = 1.00000, lambda = 0.01000, score = -1.91904\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.53857\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.35261\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.39886\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.05087\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00523\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00052\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00005\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1886.07764\n",
      "sigma = 10.00000, lambda = 0.01000, score = 402.87575\n",
      "sigma = 10.00000, lambda = 0.10000, score = 10.98566\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.52797\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52472\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37652\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08091\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00904\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00091\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 20.85918\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.45462\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50031\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50023\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49608\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37493\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49995\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 10000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 10000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 100000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 100000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 10000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 10000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 10000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 10000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 10000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 10000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 10000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 10000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10000000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 100000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 100000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 100000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 100000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 100000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 100000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 100000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000000000.00000, lambda = 1.00000, score = -0.49994\n",
      "sigma = 1000000000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000000000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000000000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000000000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000000000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000000000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000000000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000000000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000000000.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.100.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -0.99\n",
      "Approximate alpha-relative KL-divergence = 1.29\n",
      "RuLSIF completed.\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "sigma:  0.001\n",
      "lda:  0.01\n",
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.00100, lambda = 0.00100, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 0.10000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 1000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 10000000.00000, score = 0.00000\n",
      "sigma = 0.00100, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.00100, lambda = 1000000000.00000, score = 0.00000\n",
      "sigma = 0.01000, lambda = 0.00100, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.01000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 0.00100, score = -0.00042\n",
      "sigma = 0.10000, lambda = 0.01000, score = -0.00004\n",
      "sigma = 0.10000, lambda = 0.10000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 0.10000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.00100, score = 28.70297\n",
      "sigma = 1.00000, lambda = 0.01000, score = -1.56904\n",
      "sigma = 1.00000, lambda = 0.10000, score = -2.85820\n",
      "sigma = 1.00000, lambda = 1.00000, score = -1.51123\n",
      "sigma = 1.00000, lambda = 10.00000, score = -0.40226\n",
      "sigma = 1.00000, lambda = 100.00000, score = -0.04921\n",
      "sigma = 1.00000, lambda = 1000.00000, score = -0.00503\n",
      "sigma = 1.00000, lambda = 10000.00000, score = -0.00050\n",
      "sigma = 1.00000, lambda = 100000.00000, score = -0.00005\n",
      "sigma = 1.00000, lambda = 1000000.00000, score = -0.00001\n",
      "sigma = 1.00000, lambda = 10000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 0.00100, score = 1973.13400\n",
      "sigma = 10.00000, lambda = 0.01000, score = 458.17791\n",
      "sigma = 10.00000, lambda = 0.10000, score = 13.48332\n",
      "sigma = 10.00000, lambda = 1.00000, score = -0.52377\n",
      "sigma = 10.00000, lambda = 10.00000, score = -0.52774\n",
      "sigma = 10.00000, lambda = 100.00000, score = -0.37798\n",
      "sigma = 10.00000, lambda = 1000.00000, score = -0.08104\n",
      "sigma = 10.00000, lambda = 10000.00000, score = -0.00905\n",
      "sigma = 10.00000, lambda = 100000.00000, score = -0.00092\n",
      "sigma = 10.00000, lambda = 1000000.00000, score = -0.00009\n",
      "sigma = 10.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 10.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 10.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 0.00100, score = 24.85998\n",
      "sigma = 100.00000, lambda = 0.01000, score = -0.43675\n",
      "sigma = 100.00000, lambda = 0.10000, score = -0.50034\n",
      "sigma = 100.00000, lambda = 1.00000, score = -0.50026\n",
      "sigma = 100.00000, lambda = 10.00000, score = -0.49611\n",
      "sigma = 100.00000, lambda = 100.00000, score = -0.37495\n",
      "sigma = 100.00000, lambda = 1000.00000, score = -0.08671\n",
      "sigma = 100.00000, lambda = 10000.00000, score = -0.00984\n",
      "sigma = 100.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 100.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 100.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 100.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 100.00000, lambda = 1000000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 0.00100, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.01000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 0.10000, score = -0.50000\n",
      "sigma = 1000.00000, lambda = 1.00000, score = -0.49995\n",
      "sigma = 1000.00000, lambda = 10.00000, score = -0.49581\n",
      "sigma = 1000.00000, lambda = 100.00000, score = -0.37491\n",
      "sigma = 1000.00000, lambda = 1000.00000, score = -0.08677\n",
      "sigma = 1000.00000, lambda = 10000.00000, score = -0.00985\n",
      "sigma = 1000.00000, lambda = 100000.00000, score = -0.00100\n",
      "sigma = 1000.00000, lambda = 1000000.00000, score = -0.00010\n",
      "sigma = 1000.00000, lambda = 10000000.00000, score = -0.00001\n",
      "sigma = 1000.00000, lambda = 100000000.00000, score = -0.00000\n",
      "sigma = 1000.00000, lambda = 1000000000.00000, score = -0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-afe327fa7d7e>\", line 45, in <module>\n",
      "    res_dr2 = dr(Y_historical_matrix, A_historical_matrix, X_seq_train, X_test, pi_evaluation_seq_train, pi_evaluation_test, classes, pi_behavior=pi_behavior_seq_train, method='Ridge')\n",
      "  File \"../cs_ope_estimator.py\", line 79, in dr\n",
      "    densratio_obj = densratio(X_evl, X_hst)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/densratio/core.py\", line 66, in densratio\n",
      "    result = RuLSIF(x, y, alpha, sigma_range, lambda_range, kernel_num, verbose)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/densratio/RuLSIF.py\", line 61, in RuLSIF\n",
      "    opt_params = search_sigma_and_lambda(x, y, alpha, centers, sigma_range, lambda_range, verbose)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/densratio/RuLSIF.py\", line 152, in search_sigma_and_lambda\n",
      "    phi_y = compute_kernel_Gaussian(y, centers, sigma)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/densratio/RuLSIF.py\", line 187, in compute_kernel_Gaussian\n",
      "    result = [[kernel_Gaussian(x, y, sigma) for y in y_list] for x in x_list]\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/densratio/RuLSIF.py\", line 187, in <listcomp>\n",
      "    result = [[kernel_Gaussian(x, y, sigma) for y in y_list] for x in x_list]\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/densratio/RuLSIF.py\", line 187, in <listcomp>\n",
      "    result = [[kernel_Gaussian(x, y, sigma) for y in y_list] for x in x_list]\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/densratio/RuLSIF.py\", line 194, in kernel_Gaussian\n",
      "    return exp(- (norm(x - y) ** 2) / (2 * sigma * sigma))\n",
      "  File \"<__array_function__ internals>\", line 6, in norm\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\", line 2483, in norm\n",
      "    ret = sqrt(sqnorm)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/posixpath.py\", line 385, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/Users/mkato/opt/anaconda3/lib/python3.7/posixpath.py\", line 366, in normpath\n",
      "    new_comps.append(comp)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_trials = 100\n",
    "alphas = [0.7, 0.4, 0.0]\n",
    "\n",
    "tau_list = np.zeros(num_trials)\n",
    "res_ipw3_list = np.zeros((num_trials, len(alphas)))\n",
    "res_dm_list = np.zeros((num_trials, len(alphas)))\n",
    "res_dml1_truew_list = np.zeros((num_trials, len(alphas)))\n",
    "res_dml2_truew_list = np.zeros((num_trials, len(alphas)))\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    X, Y, Y_matrix, train_test_split, x_prob, classes, N, N_train, N_test = data_generation(data_name)\n",
    "\n",
    "    X_train, X_test = X[train_test_split], X[~train_test_split]\n",
    "    Y_train, Y_test = Y[train_test_split], Y[~train_test_split]\n",
    "    Y_matrix_train, Y_matrix_test = Y_matrix[train_test_split], Y_matrix[~train_test_split]\n",
    "\n",
    "    pi_behavior, pi_evaluation  = behavior_and_evaluation_policy(X, Y, train_test_split, classes, alpha=0.7)\n",
    "\n",
    "    pi_behavior_train, pi_behavior_test = pi_behavior[train_test_split], pi_behavior[~train_test_split]\n",
    "    pi_evaluation_train, pi_evaluation_test = pi_evaluation[train_test_split], pi_evaluation[~train_test_split]\n",
    "\n",
    "    tau = true_value(Y_matrix_test, pi_evaluation_test, N_test)\n",
    "\n",
    "    for idx_alpha in  range(len(alphas)):    \n",
    "        alpha = alphas[idx_alpha]\n",
    "        pi_behavior, pi_evaluation  = behavior_and_evaluation_policy(X, Y, train_test_split, classes, alpha=alpha)\n",
    "\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        X_seq_train, Y_matrix_seq_train, pi_behavior_seq_train, pi_evaluation_seq_train = X_train[perm], Y_matrix_train[perm], pi_behavior_train[perm], pi_evaluation_train[perm]\n",
    "\n",
    "        Y_historical_matrix = np.zeros(shape=(N_train, len(classes)))\n",
    "        A_historical_matrix = np.zeros(shape=(N_train, len(classes)))\n",
    "\n",
    "        for i in range(N_train):\n",
    "            a = np.random.choice(classes, p=pi_behavior[i])\n",
    "            Y_historical_matrix[i, a] = 1\n",
    "            A_historical_matrix[i, a] = 1\n",
    "            \n",
    "        #IPW3 estimator\n",
    "        res_ipw3 = ipw(Y_historical_matrix, X_seq_train, X_test, classes, pi_evaluation_train, A_hst=A_historical_matrix)\n",
    "        #Direct method estimator\n",
    "        res_dm = dm(Y_historical_matrix, X_seq_train, X_test, pi_evaluation_test, classes)\n",
    "        #DML with L1\n",
    "        res_dml1 =dml(Y_historical_matrix, A_historical_matrix, X_seq_train, X_test, pi_evaluation_seq_train, pi_evaluation_test, classes, pi_behavior=pi_behavior_seq_train, method='Lasso')\n",
    "        #DML with L2\n",
    "        res_dml2 =dml(Y_historical_matrix, A_historical_matrix, X_seq_train, X_test, pi_evaluation_seq_train, pi_evaluation_test, classes, pi_behavior=pi_behavior_seq_train, method='Ridge')\n",
    "\n",
    "        print(res_ipw3)\n",
    "        print(res_dm)\n",
    "        print(res_dr1)\n",
    "        print(res_dr2)\n",
    "        print(res_dml1_truew)\n",
    "        print(res_dml2_truew)\n",
    "        \n",
    "        res_ipw3_list[trial, idx_alpha] = res_ipw3\n",
    "        res_dm_list[trial, idx_alpha] = res_dm\n",
    "        res_dr1_list[trial, idx_alpha] = res_dr1\n",
    "        res_dr2_list[trial, idx_alpha] = res_dr2\n",
    "        res_dml1_truew_list[trial, idx_alpha] = res_dml1_truew\n",
    "        res_dml2_truew_list[trial, idx_alpha] = res_dml2_truew\n",
    "        \n",
    "        np.savetxt(\"exp_results/res_ipw3.csv\", res_ipw3_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_dm.csv\", res_dm_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_dr1.csv\", res_dr1_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_dr2.csv\", res_dr2_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_dml1.csv\", res_dml1_truew_list, delimiter=\",\")\n",
    "        np.savetxt(\"exp_results/res_dml2.csv\", res_dml2_truew_list, delimiter=\",\")\n",
    "        \n",
    "    tau_list[trial] = tau\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
